ğŸ“˜ Multi-Agent Reinforcement Learning â€“ Assignment 1

This repository contains the implementation and analysis of three Markov Decision Process (MDP) problems solved using:

âœ… Value Iteration

âœ… Policy Iteration

âœ… Monte Carlo Methods

The project focuses on robotics-oriented decision making under uncertainty.

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ marl_assignment1.py          # Main implementation
â”œâ”€â”€ assign1.pdf                  # Final report
â”œâ”€â”€ manual_derivations.pdf       # Handwritten Bellman derivations (3Ã—3 grid)
â”œâ”€â”€ convergence_plot.png
â”œâ”€â”€ orientation_aware_10.png
â”œâ”€â”€ policy_risk_0_1.png
â”œâ”€â”€ policy_risk_0_3.png
â”œâ”€â”€ ...
â””â”€â”€ README.md
ğŸ§  Problem Overview
ğŸ”¹ Question 1 â€“ Orientation-Aware Navigation

State: 
ğ‘ 
=
(
ğ‘¥
,
ğ‘¦
,
ğœƒ
)
s=(x,y,Î¸)

Actions: Forward, TurnLeft, TurnRight

Stochastic forward motion (slip probability)

Collision and goal terminal states

Key Insight:

Including orientation in the state space fundamentally changes policy structure.

ğŸ”¹ Question 2 â€“ Battery-Aware Navigation

State: 
ğ‘ 
=
(
ğ‘¥
,
ğ‘¦
,
ğ‘
)
s=(x,y,b)

Recharge action available at charging stations

Terminal failure if battery depletes

Key Insight:

Battery-awareness emerges naturally from reward optimization.

ğŸ”¹ Question 3 â€“ Risk-Sensitive Navigation

Hazard zones with slip probability

Catastrophic penalty (-200)

Risk-adjusted decision-making

Key Insight:

Shortest path is not always optimal under stochastic risk.

âš™ï¸ Algorithms Implemented
1ï¸âƒ£ Value Iteration

Bellman Optimality Update:

ğ‘‰
ğ‘˜
+
1
(
ğ‘ 
)
=
max
â¡
ğ‘
âˆ‘
ğ‘ 
â€²
ğ‘ƒ
(
ğ‘ 
â€²
âˆ£
ğ‘ 
,
ğ‘
)
[
ğ‘…
+
ğ›¾
ğ‘‰
ğ‘˜
(
ğ‘ 
â€²
)
]
V
k+1
	â€‹

(s)=
a
max
	â€‹

s
â€²
âˆ‘
	â€‹

P(s
â€²
âˆ£s,a)[R+Î³V
k
	â€‹

(s
â€²
)]

Converges in 89 iterations (10Ã—10 grid)

Memory efficient

Simpler update rule

2ï¸âƒ£ Policy Iteration

Alternates between:

Policy Evaluation

Policy Improvement

Converges in 9 improvement steps

Higher memory usage (stores value + policy)

Fewer outer iterations but heavier computation per step

3ï¸âƒ£ Monte Carlo

Model-free

Requires many episodes

Useful when transition model is unknown

ğŸ“Š Experimental Results (10Ã—10 Grid)
Method	Iterations	Runtime (sec)	Memory (bytes)
Value Iteration	89	~0.15	51968
Policy Iteration	9	~0.34	113486
Observations:

VI requires more iterations but runs faster.

PI converges in fewer steps but consumes more memory.

Model-based methods outperform Monte Carlo in convergence speed.

ğŸ“ˆ Convergence Behavior

Convergence plots are included for:

Q1 (Orientation-aware)

Q2 (Battery-aware)

Q3 (Risk-sensitive)

Bellman error decreases monotonically, confirming correctness.

ğŸ§ª Manual Verification

To validate correctness:

A reduced 3Ã—3 grid was used.

First three Bellman updates were computed manually.

Handwritten derivations are included in manual_derivations.pdf.

This verifies correctness before scaling to the full 10Ã—10 grid.

ğŸ—ï¸ How to Run
python marl_assignment1.py

The script:

Runs Value Iteration

Runs Policy Iteration

Compares runtime & memory

Generates policy visualizations

Plots convergence graphs

ğŸ¯ Key Takeaways

State representation critically affects optimal policy.

Long-term penalties significantly alter behavior.

Risk and uncertainty invalidate shortest-path intuition.

Reward design strongly influences emergent behavior.

ğŸ“š Course

Multi-Agent Reinforcement Learning
Assignment 1
